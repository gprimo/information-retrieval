{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trabalho3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMQvY2YRAobOZm/uY6pSGle",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gprimo/information-retrieval/blob/main/src/trabalho3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIf_eMG0P129",
        "outputId": "da463d61-f4df-46ed-bbb8-1f97505d34ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.sax as sax\n",
        "\n",
        "class QueryNumberHandler(sax.ContentHandler) :\n",
        "\n",
        "  def startElement(self, name, attrs) :\n",
        "    self.current = name\n",
        "\n",
        "  def characters(self, content) :\n",
        "    if self.current == \"QueryNumber\":\n",
        "      self.QueryNumber = content\n",
        "\n",
        "\n",
        "  def endElement(self, name) :\n",
        "    if self.current == \"QueryNumber\":\n",
        "      querynumberlist.append(self.QueryNumber)\n",
        "    self.current = \"\""
      ],
      "metadata": {
        "id": "HrF3sSJUya0v"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.sax as sax\n",
        "\n",
        "class QueryTextHandler(sax.ContentHandler) :\n",
        "\n",
        "  def startElement(self, name, attrs) :\n",
        "    self.current = name\n",
        "\n",
        "  def characters(self, content) :\n",
        "    if self.current == \"QueryText\":\n",
        "      self.QueryText = content\n",
        "\n",
        "\n",
        "  def endElement(self, name) :\n",
        "    if self.current == \"QueryText\":\n",
        "      queryTextlist.append(self.QueryText)\n",
        "\n",
        "    self.current = \"\""
      ],
      "metadata": {
        "id": "ydlwJP3u8Vpe"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "\n",
        "f = open(\"/content/drive/MyDrive/Colab Notebooks/Data/PC.CFG\", \"r\")\n",
        "cfg = f.readlines()\n",
        "\n",
        "#query.xml esta aqui em read\n",
        "r = cfg[0].split('= ',1)[1]\n",
        "read = r.split('\\n', 1)[0]\n",
        "\n",
        "#aqui é a diretiva pra criar o primeiro csv\n",
        "q = cfg[1].split('= ',1)[1]\n",
        "query = q.split('\\n', 1)[0]\n",
        "\n",
        "\n",
        "#aqui é a diretiva pra criar o segundo csv\n",
        "d = cfg[2].split('= ',1)[1]\n",
        "finalDocument = d.split('\\n', 1)[0]\n",
        "print(finalDocument)\n",
        "\n",
        "\n",
        "g = open(read, 'r')\n",
        "queryCreate = open(query, 'w+')\n",
        "voteDocumentCreate = open(finalDocument, 'w+')\n",
        "querynumberlist = []\n",
        "queryTextlist = []\n",
        "queryItemlist = []\n",
        "document = []\n",
        "queryNumber = []\n",
        "vote = []\n",
        "\n",
        "handler = QueryNumberHandler()\n",
        "parser = sax.make_parser()\n",
        "parser.setContentHandler(handler)\n",
        "parser.parse(read)\n",
        "\n",
        "handlerText = QueryTextHandler()\n",
        "parserText = sax.make_parser()\n",
        "parserText.setContentHandler(handlerText)\n",
        "parserText.parse(read)\n",
        "\n",
        "tree = ET.parse(read)\n",
        "root = tree.getroot()\n",
        "queryDocumentVotes = {}\n",
        "votes = 0\n",
        "iter = 1\n",
        "docuIndex = '1'\n",
        "\n",
        "for child in root:\n",
        "  for item in child:\n",
        "    if (item.tag == 'QueryNumber'):\n",
        "      queryDocumentVotes[item.text] = [votes, []]\n",
        "  \n",
        "\n",
        "for child in root:\n",
        "  for item in child:\n",
        "    if (item.tag == 'QueryNumber'):\n",
        "      query = item.text\n",
        "    if (item.tag == 'Records'):\n",
        "      for x in item: \n",
        "        sc = x.attrib['score']\n",
        "        for key in queryDocumentVotes:\n",
        "          if (int(key) == int(sc)):\n",
        "            queryDocumentVotes[key][1].append(docuIndex)\n",
        "            queryDocumentVotes[key] = [queryDocumentVotes[key][0] + 1, queryDocumentVotes[key][1]]\n",
        "  iter = iter + 1\n",
        "  docuIndex = str(iter)\n",
        "\n",
        "i = 0\n",
        "for item in querynumberlist:\n",
        "  queryCreate.write(item + ';' + queryTextlist[i] + '\\n')\n",
        "  i = i + 1\n",
        "\n",
        "\n",
        "for key in queryDocumentVotes:\n",
        "  j = 0\n",
        "  for item in queryDocumentVotes[key][1]:\n",
        "    voteDocumentCreate.write(key + ';' + item + ';' + str(queryDocumentVotes[key][0]) + '\\n')\n",
        "    j = j + 1\n",
        "  \n"
      ],
      "metadata": {
        "id": "iZhh3UjRVvWy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "770ee039-5009-4f64-c076-646218625620"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Data/queryout.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def BaseConvertXML(xml, base) :\n",
        "\n",
        "  tree = ET.parse(xml)\n",
        "  root = tree.getroot()\n",
        "\n",
        "  tree = ET.parse(xml)\n",
        "  root = tree.getroot()\n",
        "\n",
        "  for record in root.iter('ABSTRACT'):\n",
        "    base.append(record.text)\n",
        "  "
      ],
      "metadata": {
        "id": "D9NNbE8VRS-e"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"/content/drive/MyDrive/Colab Notebooks/Data/GLI.CFG\", \"r\")\n",
        "cfg = f.readlines()\n",
        "\n",
        "#primeiro xml esta aqui em read\n",
        "r = cfg[0].split('= ',1)[1]\n",
        "read1 = r.split('\\n', 1)[0]\n",
        "\n",
        "#segunda xml esta aqui em read2\n",
        "r = cfg[1].split('= ',1)[1]\n",
        "read2 = r.split('\\n', 1)[0]\n",
        "\n",
        "#terceiro xml esta aqui em read3\n",
        "r = cfg[2].split('= ',1)[1]\n",
        "read3 = r.split('\\n', 1)[0]\n",
        "\n",
        "#quarto xml esta aqui em read4\n",
        "r = cfg[3].split('= ',1)[1]\n",
        "read4 = r.split('\\n', 1)[0]\n",
        "\n",
        "#quinto xml esta aqui em read5\n",
        "r = cfg[4].split('= ',1)[1]\n",
        "read5 = r.split('\\n', 1)[0]\n",
        "\n",
        "#sexto xml esta aqui em read6\n",
        "r = cfg[5].split('= ',1)[1]\n",
        "read6 = r.split('\\n', 1)[0]\n",
        "\n",
        "#aqui é a diretiva pra criar a lista invertida\n",
        "d = cfg[6].split('= ',1)[1]\n",
        "inverse = d.split('\\n', 1)[0]\n",
        "\n",
        "g = open(read, 'r')\n",
        "queryCreate = open(inverse, 'w+')\n",
        "print(inverse)\n",
        "\n",
        "base1 = []\n",
        "base1opt = []\n",
        "base2 = []\n",
        "base2opt = []\n",
        "base3 = []\n",
        "base3opt = []\n",
        "base4 = []\n",
        "base4opt = []\n",
        "base5 = []\n",
        "base5opt = []\n",
        "base6 = []\n",
        "base6opt = []\n",
        "BaseConvertXML(read1, base1)\n",
        "BaseConvertXML(read2, base2)\n",
        "BaseConvertXML(read3, base3)\n",
        "BaseConvertXML(read4, base4)\n",
        "BaseConvertXML(read5, base5)\n",
        "BaseConvertXML(read6, base6)\n",
        "\n",
        "lista_invertida = {}\n",
        "lista_invertida2 = {}\n",
        "lista_invertida3 = {}\n",
        "lista_invertida4 = {}\n",
        "lista_invertida5 = {}\n",
        "lista_invertida6 = {}\n",
        "\n",
        "dataset = []\n",
        "\n",
        "\n",
        "for word in base1:\n",
        "  wordopt = word.replace(\",\",\"\")\n",
        "  wordopt = word.replace(\"\\n\",\" \")\n",
        "  base1opt.append(wordopt)\n",
        "  dataset.append(wordopt)\n",
        "\n",
        "\n",
        "for word in base2:\n",
        "  wordopt2 = word.replace(\",\",\"\")\n",
        "  wordopt2 = word.replace(\"\\n\",\" \")\n",
        "  base2opt.append(wordopt2)\n",
        "  dataset.append(wordopt2)\n",
        "\n",
        "for word in base3:\n",
        "  wordopt3 = word.replace(\",\",\"\")\n",
        "  wordopt3 = word.replace(\"\\n\",\" \")\n",
        "  base3opt.append(wordopt3)\n",
        "  dataset.append(wordopt3)\n",
        "\n",
        "for word in base4:\n",
        "  wordopt4 = word.replace(\",\",\"\")\n",
        "  wordopt4 = word.replace(\"\\n\",\" \")\n",
        "  base4opt.append(wordopt4)\n",
        "  dataset.append(wordopt4)\n",
        "\n",
        "for word in base5:\n",
        "  wordopt5 = word.replace(\",\",\"\")\n",
        "  wordopt5 = word.replace(\"\\n\",\" \")\n",
        "  base5opt.append(wordopt5)\n",
        "  dataset.append(wordopt5)\n",
        "\n",
        "for word in base6:\n",
        "  wordopt6 = word.replace(\",\",\"\")\n",
        "  wordopt6 = word.replace(\"\\n\",\" \")\n",
        "  base6opt.append(wordopt6)\n",
        "  dataset.append(wordopt6)\n",
        "\n",
        "\n",
        "for i in range(len(base1opt)):\n",
        "  fraselc = str.upper(base1opt[i]).split ()\n",
        "  for palavra in fraselc:\n",
        "    docs = lista_invertida.get(palavra, set())\n",
        "    docs.add(i+1)\n",
        "    lista_invertida[palavra] = docs\n",
        "\n",
        "for i in range(len(base2opt)):\n",
        "  fraselc = str.upper(base2opt[i]).split ()\n",
        "  for palavra in fraselc:\n",
        "    docs = lista_invertida2.get(palavra, set())\n",
        "    docs.add(i+1)\n",
        "    lista_invertida2[palavra] = docs\n",
        "\n",
        "for i in range(len(base3opt)):\n",
        "  fraselc = str.upper(base3opt[i]).split ()\n",
        "  for palavra in fraselc:\n",
        "    docs = lista_invertida3.get(palavra, set())\n",
        "    docs.add(i+1)\n",
        "    lista_invertida3[palavra] = docs\n",
        "\n",
        "for i in range(len(base4opt)):\n",
        "  fraselc = str.upper(base4opt[i]).split ()\n",
        "  for palavra in fraselc:\n",
        "    docs = lista_invertida4.get(palavra, set())\n",
        "    docs.add(i+1)\n",
        "    lista_invertida4[palavra] = docs\n",
        "\n",
        "for i in range(len(base5opt)):\n",
        "  fraselc = str.upper(base5opt[i]).split ()\n",
        "  for palavra in fraselc:\n",
        "    docs = lista_invertida5.get(palavra, set())\n",
        "    docs.add(i+1)\n",
        "    lista_invertida5[palavra] = docs\n",
        "\n",
        "for i in range(len(base6opt)):\n",
        "  fraselc = str.upper(base6opt[i]).split ()\n",
        "  for palavra in fraselc:\n",
        "    docs = lista_invertida6.get(palavra, set())\n",
        "    docs.add(i+1)\n",
        "    lista_invertida6[palavra] = docs\n",
        "\n",
        "\n",
        "chaves = [\"FIBROSIS\",\"BLOOD\",\"CHILDREN\",\"CYSTIC\",\"DESCRIBED\",\"DETERMINED\",\"DIAGNOSIS\",\"HUMAN\",\"INFECTION\",\"LEVELS\",\"METHOD\",\"PSEUDOMONAS\",\"RESPIRATORY\", \"CHILD\" ,\"SERUM\", \"SALIVA\", \"ACID\"]\n",
        "chaves.sort()\n",
        "for i in chaves:\n",
        "  queryCreate.write(i + \";\" + str(len(lista_invertida[i])) + ';' + str(len(lista_invertida2[i])) + ';'+ str(len(lista_invertida3[i])) +';'+ str(len(lista_invertida4[i])) + ';'+ str(len(lista_invertida5[i])) + ';' + str(len(lista_invertida6[i])) + '\\n')\n",
        "  #queryCreate.write(i + \";\" + len(lista_invertida[i])) +  len(lista_invertida2[i]), len(lista_invertida3[i]), len(lista_invertida4[i]), len(lista_invertida5[i]), len(lista_invertida6[i]) )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GRaf2QVg5b_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6ce6b74-e989-404d-8544-f5bce7ca9058"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Data/listainvertida.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kENkTGBw06X4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "f = open(\"/content/drive/MyDrive/Colab Notebooks/Data/INDEX.CFG\", \"r\")\n",
        "cfg = f.readlines()\n",
        "listainvertida = {}\n",
        "\n",
        "#primeiro xml esta aqui em read\n",
        "r = cfg[0].split('= ',1)[1]\n",
        "read = r.split('\\n', 1)[0]\n",
        "\n",
        "\n",
        "with open (read,\"r\") as file:\n",
        "  file_csv = csv.reader(file, delimiter=\";\")\n",
        "  for i, line in enumerate(file_csv):\n",
        "    print(line)\n",
        "    listainvertida[line[0]]=int(line[1])+int(line[2])+int(line[3])+int(line[4])+int(line[5])+int(line[6])\n"
      ],
      "metadata": {
        "id": "gKTI_-ZiKh9k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "999dad20-4674-4f33-aa42-a37f0bffa9e1"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ACID', '10', '3', '14', '11', '10', '20']\n",
            "['BLOOD', '8', '13', '12', '7', '7', '9']\n",
            "['CHILD', '3', '5', '6', '6', '4', '3']\n",
            "['CHILDREN', '29', '28', '30', '30', '23', '31']\n",
            "['CYSTIC', '83', '102', '131', '123', '120', '134']\n",
            "['DESCRIBED', '6', '7', '5', '4', '4', '7']\n",
            "['DETERMINED', '7', '6', '5', '5', '9', '6']\n",
            "['DIAGNOSIS', '8', '11', '9', '15', '15', '6']\n",
            "['FIBROSIS', '73', '89', '123', '108', '105', '118']\n",
            "['HUMAN', '8', '5', '15', '14', '9', '12']\n",
            "['INFECTION', '7', '6', '6', '10', '5', '6']\n",
            "['LEVELS', '16', '16', '21', '6', '13', '19']\n",
            "['METHOD', '14', '5', '7', '17', '10', '6']\n",
            "['PSEUDOMONAS', '15', '8', '9', '5', '9', '14']\n",
            "['RESPIRATORY', '13', '10', '10', '10', '10', '8']\n",
            "['SALIVA', '5', '5', '12', '4', '3', '1']\n",
            "['SERUM', '14', '28', '26', '23', '20', '32']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# biblioteca para ordenação\n",
        "import heapq\n",
        "print(listainvertida)\n",
        "\n",
        "# ordena a lista para saber qual é a palavra que mais se repete\n",
        "freq_words = heapq.nlargest(50,listainvertida, key=listainvertida.get)\n",
        "print(freq_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBvTIHj7txKc",
        "outputId": "57bd3e6e-8f47-461a-d1e8-3da6236e0ac9"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ACID': 68, 'BLOOD': 56, 'CHILD': 27, 'CHILDREN': 171, 'CYSTIC': 693, 'DESCRIBED': 33, 'DETERMINED': 38, 'DIAGNOSIS': 64, 'FIBROSIS': 616, 'HUMAN': 63, 'INFECTION': 40, 'LEVELS': 91, 'METHOD': 59, 'PSEUDOMONAS': 60, 'RESPIRATORY': 61, 'SALIVA': 30, 'SERUM': 143}\n",
            "['CYSTIC', 'FIBROSIS', 'CHILDREN', 'SERUM', 'LEVELS', 'ACID', 'DIAGNOSIS', 'HUMAN', 'RESPIRATORY', 'PSEUDOMONAS', 'METHOD', 'BLOOD', 'INFECTION', 'DETERMINED', 'DESCRIBED', 'SALIVA', 'CHILD']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  base1optupper = []\n",
        "  base2optupper = []\n",
        "  base3optupper = []\n",
        "  base4optupper = []\n",
        "  base5optupper = []\n",
        "  base6optupper = []\n",
        "\n",
        "  for data in base1opt:\n",
        "    base1optupper.append(data.upper())\n",
        "\n",
        "  for data in base2opt:\n",
        "    base2optupper.append(data.upper())\n",
        "\n",
        "  for data in base3opt:\n",
        "    base3optupper.append(data.upper())\n",
        "\n",
        "  for data in base4opt:\n",
        "    base4optupper.append(data.upper())\n",
        "\n",
        "  for data in base5opt:\n",
        "    base5optupper.append(data.upper())\n",
        "\n",
        "  for data in base6opt:\n",
        "    base6optupper.append(data.upper())"
      ],
      "metadata": {
        "id": "UH8VkxuV7tI8"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# criando o IDF\n",
        "import numpy as np \n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "\n",
        "def idf_create(query):\n",
        "\n",
        "  word_idfs = {}\n",
        "  for word in query:\n",
        "      doc_count = 0 \n",
        "      for data in base1optupper:\n",
        "          if word in nltk.word_tokenize(data):\n",
        "              doc_count += 1  \n",
        "      for data in base2optupper:\n",
        "        if word in nltk.word_tokenize(data):\n",
        "            doc_count += 1    \n",
        "      for data in base3optupper:\n",
        "        if word in nltk.word_tokenize(data):\n",
        "            doc_count += 1  \n",
        "      for data in base4optupper:\n",
        "        if word in nltk.word_tokenize(data):\n",
        "            doc_count += 1   \n",
        "      for data in base5optupper:\n",
        "        if word in nltk.word_tokenize(data):\n",
        "            doc_count += 1 \n",
        "      for data in base6optupper:\n",
        "        if word in nltk.word_tokenize(data):\n",
        "            doc_count += 1             \n",
        "      word_idfs[word] = np.log((6/doc_count)+1) \n",
        "  return word_idfs   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b72cQvS0t4Qj",
        "outputId": "762d6861-bc4e-4856-988c-45d934dd15c6"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idf = idf_create(freq_words)\n",
        "print(idf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGM2z6U2Xdpq",
        "outputId": "01810b0e-00f1-4dbd-aef2-559ceb4386a6"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'CYSTIC': 0.008583743691391435, 'FIBROSIS': 0.008571481050140963, 'CHILDREN': 0.030930300691358558, 'SERUM': 0.040005334613699206, 'LEVELS': 0.05770831762064677, 'ACID': 0.0750351859429141, 'DIAGNOSIS': 0.07796154146971192, 'HUMAN': 0.09097177820572679, 'RESPIRATORY': 0.09237332013101507, 'PSEUDOMONAS': 0.09531017980432493, 'METHOD': 0.08223809823697201, 'BLOOD': 0.09531017980432493, 'INFECTION': 0.10724553035359759, 'DETERMINED': 0.11551288712184424, 'DESCRIBED': 0.11778303565638346, 'SALIVA': 0.1582240052148942, 'CHILD': 0.17185025692665923}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_create(query):\n",
        "\n",
        "  tf_matrix = {}\n",
        "\n",
        "  for word in query:\n",
        "    doc_tf = []\n",
        "    for data in base1optupper:\n",
        "      frequency = 0\n",
        "      for w in nltk.word_tokenize(data):\n",
        "          if w == word:\n",
        "            frequency += 1\n",
        "      tf_word = frequency/len(nltk.word_tokenize(data))\n",
        "    doc_tf.append(tf_word)\n",
        "\n",
        "    for data in base2optupper:\n",
        "      frequency = 0\n",
        "      for w in nltk.word_tokenize(data):\n",
        "          if w == word:\n",
        "            frequency += 1\n",
        "      tf_word = frequency/len(nltk.word_tokenize(data))\n",
        "    doc_tf.append(tf_word)\n",
        "    for data in base3optupper:\n",
        "      frequency = 0\n",
        "      for w in nltk.word_tokenize(data):\n",
        "          if w == word:\n",
        "            frequency += 1\n",
        "      tf_word = frequency/len(nltk.word_tokenize(data))\n",
        "    doc_tf.append(tf_word)\n",
        "    for data in base4optupper:\n",
        "      frequency = 0\n",
        "      for w in nltk.word_tokenize(data):\n",
        "          if w == word:\n",
        "            frequency += 1\n",
        "      tf_word = frequency/len(nltk.word_tokenize(data))\n",
        "    doc_tf.append(tf_word)\n",
        "    for data in base5optupper:\n",
        "      frequency = 0\n",
        "      for w in nltk.word_tokenize(data):\n",
        "          if w == word:\n",
        "            frequency += 1\n",
        "      tf_word = frequency/len(nltk.word_tokenize(data))\n",
        "    doc_tf.append(tf_word)\n",
        "    for data in base6optupper:\n",
        "      frequency = 0\n",
        "      for w in nltk.word_tokenize(data):\n",
        "          if w == word:\n",
        "            frequency += 1\n",
        "      tf_word = frequency/len(nltk.word_tokenize(data))\n",
        "    doc_tf.append(tf_word)\n",
        "\n",
        "    tf_matrix[word] = doc_tf\n",
        "  \n",
        "  return tf_matrix  \n",
        "\n"
      ],
      "metadata": {
        "id": "cbBdPWIRxsUj"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = tf_create(freq_words)\n",
        "print(matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnDM-YqkYMRw",
        "outputId": "fa449ea9-95c2-4bad-ed85-b188fb5578d2"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'CYSTIC': [0.015384615384615385, 0.005813953488372093, 0.0, 0.05555555555555555, 0.0, 0.02040816326530612], 'FIBROSIS': [0.015384615384615385, 0.005813953488372093, 0.0, 0.05555555555555555, 0.0, 0.02040816326530612], 'CHILDREN': [0.0, 0.011627906976744186, 0.030303030303030304, 0.0, 0.006666666666666667, 0.006802721088435374], 'SERUM': [0.0, 0.029069767441860465, 0.0, 0.0, 0.0, 0.027210884353741496], 'LEVELS': [0.0, 0.0, 0.0, 0.0, 0.0, 0.02040816326530612], 'ACID': [0.0, 0.0, 0.0, 0.0, 0.0, 0.02040816326530612], 'DIAGNOSIS': [0.0, 0.005813953488372093, 0.030303030303030304, 0.0, 0.0, 0.0], 'HUMAN': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'RESPIRATORY': [0.0, 0.0, 0.0, 0.0, 0.02, 0.0], 'PSEUDOMONAS': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'METHOD': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'BLOOD': [0.015384615384615385, 0.0, 0.0, 0.0, 0.0, 0.0], 'INFECTION': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'DETERMINED': [0.0, 0.005813953488372093, 0.0, 0.0, 0.0, 0.0], 'DESCRIBED': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'SALIVA': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'CHILD': [0.0, 0.005813953488372093, 0.0, 0.0, 0.0, 0.0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF cálculo\n",
        "import numpy as np \n",
        "\n",
        "def tfidf_create(tfmatrix, idfarray):\n",
        "  tfidf_matrix = []\n",
        "\n",
        "  for word in tfmatrix.keys():\n",
        "      tfidf = []\n",
        "      for value in tfmatrix[word]:\n",
        "          score = value * idfarray[word]\n",
        "          tfidf.append(score)\n",
        "      tfidf_matrix.append(tfidf)\n",
        "  \n",
        "  np.set_printoptions(suppress=True)\n",
        "  arr = np.array(tfidf_matrix)\n",
        "\n",
        "  return arr\n"
      ],
      "metadata": {
        "id": "CaSabo5yyPKu"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidfmatrix = tfidf_create(matrix, idf)\n",
        "print(tfidfmatrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAtbB3JeYgfH",
        "outputId": "4f436921-8baa-41e8-eb5d-3c20672b779b"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.00013206 0.00004991 0.         0.00047687 0.         0.00017518]\n",
            " [0.00013187 0.00004983 0.         0.00047619 0.         0.00017493]\n",
            " [0.         0.00035965 0.00093728 0.         0.0002062  0.00021041]\n",
            " [0.         0.00116295 0.         0.         0.         0.00108858]\n",
            " [0.         0.         0.         0.         0.         0.00117772]\n",
            " [0.         0.         0.         0.         0.         0.00153133]\n",
            " [0.         0.00045326 0.00236247 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.00184747 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.        ]\n",
            " [0.00146631 0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.00067159 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.00099913 0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "f = open(\"/content/drive/MyDrive/Colab Notebooks/Data/BUSCAR.CFG\", \"r\")\n",
        "cfg = f.readlines()\n",
        "\n",
        "r = cfg[0].split('= ',1)[1]\n",
        "query = r.split('\\n', 1)[0]\n",
        "\n",
        "r = cfg[1].split('= ',1)[1]\n",
        "busca = r.split('\\n', 1)[0]\n",
        "\n",
        "g = open(query, 'r')\n",
        "queryCreate = open(busca, 'w+')\n",
        "queries = {}\n",
        "queryTerms = []\n",
        "queryTermstokenize = []\n",
        "finalTermsQuery = []\n",
        "queriesUpper = []\n",
        "\n",
        "\n",
        "with open (query,\"r\") as file:\n",
        "  file_csv = csv.reader(file, delimiter=\";\")\n",
        "  for i, line in enumerate(file_csv):\n",
        "    queries[line[0]] = line[1]\n",
        "\n",
        "for key, value in queries.items() :\n",
        "  queryTerms.append(value)\n",
        "\n",
        "for word in queryTerms:\n",
        "  tokenTerm = nltk.word_tokenize(word)\n",
        "  queryTermstokenize.append(tokenTerm)\n",
        "\n",
        "for term in queryTermstokenize:\n",
        "  for x in term:\n",
        "    if len(x) > 3:\n",
        "      finalTermsQuery.append(x)\n",
        "\n",
        "for word in finalTermsQuery:\n",
        "  upperTerm = str.upper(word)\n",
        "  queriesUpper.append(upperTerm)\n",
        "\n",
        "queriesUpper.sort()\n",
        "print(queriesUpper)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3uNYAeieOII",
        "outputId": "c12576a6-bcda-451c-dd05-fed051fac8e8"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AERUGINOSA', 'ALBUMIN', 'ANTIMICROBIAL', 'EOSINOPHILIA', 'HETEROZYGOUS', 'HOMOZYGOUS', 'INFECTION', 'MECONIUM', 'NASAL', 'PERIPHERAL', 'PSEUDOMONAS', 'SHWACHMAN-KULCZYCKI', 'THERAPY']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reverselist = {}\n",
        "queriesUpper.sort()\n",
        "\n",
        "\n",
        "\n",
        "for item in queriesUpper:\n",
        "  if lista_invertida[item] == 0:\n",
        "    reverselist[item] = 0\n",
        "  else:\n",
        "    reverselist[item] = len(lista_invertida[item])\n",
        "\n",
        "\n",
        "print(reverselist)\n",
        "\n",
        "freqwords = heapq.nlargest(50,reverselist, key=reverselist.get)\n",
        "print(freqwords)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isaoQAr8Akcj",
        "outputId": "caaaf51a-a12a-4715-a276-dde6e00e7e2b"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'AERUGINOSA': 9, 'ALBUMIN': 4, 'ANTIMICROBIAL': 0, 'EOSINOPHILIA': 0, 'HETEROZYGOUS': 0, 'HOMOZYGOUS': 1, 'INFECTION': 7, 'MECONIUM': 4, 'NASAL': 3, 'PERIPHERAL': 3, 'PSEUDOMONAS': 15, 'SHWACHMAN-KULCZYCKI': 0, 'THERAPY': 9}\n",
            "['PSEUDOMONAS', 'AERUGINOSA', 'THERAPY', 'INFECTION', 'ALBUMIN', 'MECONIUM', 'NASAL', 'PERIPHERAL', 'HOMOZYGOUS', 'ANTIMICROBIAL', 'EOSINOPHILIA', 'HETEROZYGOUS', 'SHWACHMAN-KULCZYCKI']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idf = idf_create(freqwords)\n",
        "print(idf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMQUWqR8wF_M",
        "outputId": "8714c7bc-1fc7-40f5-a657-c772510ffdc8"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'PSEUDOMONAS': 0.09531017980432493, 'AERUGINOSA': 0.09381875521765491, 'THERAPY': 0.09237332013101507, 'INFECTION': 0.10724553035359759, 'ALBUMIN': 0.23180161405732438, 'MECONIUM': 0.1278333715098848, 'NASAL': 0.3566749439387324, 'PERIPHERAL': 0.4054651081081644, 'HOMOZYGOUS': 0.3566749439387324, 'ANTIMICROBIAL': 0.6931471805599453, 'EOSINOPHILIA': 1.0986122886681098, 'HETEROZYGOUS': 0.26236426446749106, 'SHWACHMAN-KULCZYCKI': 0.9162907318741551}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = tf_create(freqwords)\n",
        "print(matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvuzp1sbsGIc",
        "outputId": "db8ec43f-e467-45fb-be97-7f9008b39f60"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'PSEUDOMONAS': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'AERUGINOSA': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'THERAPY': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'INFECTION': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'ALBUMIN': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'MECONIUM': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'NASAL': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'PERIPHERAL': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'HOMOZYGOUS': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'ANTIMICROBIAL': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'EOSINOPHILIA': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'HETEROZYGOUS': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'SHWACHMAN-KULCZYCKI': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}\n"
          ]
        }
      ]
    }
  ]
}